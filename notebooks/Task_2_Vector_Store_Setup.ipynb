{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Task 2: Text Chunking, Embedding, and Vector Store Indexing\n",
                "\n",
                "## Objective\n",
                "Convert the cleaned text narratives into a format suitable for efficient semantic search, ensuring proportional representation across 5 product categories.\n",
                "\n",
                "## Final Workflow\n",
                "1. **Load Raw Data**: Re-load to capture the full range of product categories.\n",
                "2. **Target Products**: Filter for the 5 most common/relevant categories:\n",
                "    - Credit card or prepaid card\n",
                "    - Checking or savings account\n",
                "    - Payday/Personal loans\n",
                "    - Money transfers\n",
                "    - Debt collection\n",
                "3. **Stratified Sampling**: 3,000 complaints per product (Total ~15,000).\n",
                "4. **Text Chunking**: Recursive character splitting (500/50).\n",
                "5. **Embeddings**: `all-MiniLM-L6-v2`.\n",
                "6. **Indexing**: FAISS FlatL2."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import re\n",
                "import os\n",
                "import pickle\n",
                "import faiss\n",
                "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
                "from sentence_transformers import SentenceTransformer\n",
                "\n",
                "# File paths\n",
                "raw_data_path = \"../data/raw/complaints.csv\"\n",
                "vector_store_dir = \"../vector_store/\"\n",
                "os.makedirs(vector_store_dir, exist_ok=True)\n",
                "\n",
                "def clean_text(text):\n",
                "    if not isinstance(text, str): return \"\"\n",
                "    text = text.lower()\n",
                "    text = re.sub(r'[^a-zA-Z0-9\\s.,!?]', '', text)\n",
                "    text = re.sub(r'\\s+', ' ', text).strip()\n",
                "    return text"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load and Filter Data\n",
                "We load the raw data and filter for the 5 categories to ensure balanced representation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "product_map = {\n",
                "    'Credit card or prepaid card': 'Credit card',\n",
                "    'Checking or savings account': 'Savings account',\n",
                "    'Payday loan, title loan, personal loan, or advance loan': 'Personal loan',\n",
                "    'Money transfer, virtual currency, or money service': 'Money transfers',\n",
                "    'Debt collection': 'Debt collection'\n",
                "}\n",
                "\n",
                "print(\"Loading and filtering data...\")\n",
                "cols = ['Complaint ID', 'Product', 'Consumer complaint narrative']\n",
                "df = pd.read_csv(raw_data_path, usecols=cols)\n",
                "df = df[df['Product'].isin(product_map.keys())].copy()\n",
                "df['Product'] = df['Product'].map(product_map)\n",
                "df = df.dropna(subset=['Consumer complaint narrative'])\n",
                "\n",
                "print(f\"Available complaints with narratives: {len(df)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Stratified Sampling\n",
                "Goal: 15,000 complaints (3,000 per product)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "target_sample_size = 15000\n",
                "samples_per_product = target_sample_size // 5\n",
                "\n",
                "df_sampled = df.groupby('Product', group_keys=False).apply(\n",
                "    lambda x: x.sample(n=min(len(x), samples_per_product), random_state=42)\n",
                ")\n",
                "\n",
                "print(f\"Sampled {len(df_sampled)} complaints.\")\n",
                "print(df_sampled['Product'].value_counts())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Pre-processing and Chunking"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df_sampled['cleaned_narrative'] = df_sampled['Consumer complaint narrative'].apply(clean_text)\n",
                "\n",
                "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
                "chunks = []\n",
                "metadata = []\n",
                "\n",
                "for idx, row in df_sampled.iterrows():\n",
                "    doc_chunks = text_splitter.split_text(row['cleaned_narrative'])\n",
                "    for chunk in doc_chunks:\n",
                "        chunks.append(chunk)\n",
                "        metadata.append({\n",
                "            'complaint_id': row['Complaint ID'],\n",
                "            'product': row['Product'],\n",
                "            'original_text': row['Consumer complaint narrative']\n",
                "        })\n",
                "\n",
                "print(f\"Generated {len(chunks)} chunks.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Embedding Generation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
                "embeddings = model.encode(chunks, show_progress_bar=True)\n",
                "print(f\"Embeddings shape: {embeddings.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Persistence"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "dimension = embeddings.shape[1]\n",
                "index = faiss.IndexFlatL2(dimension)\n",
                "index.add(np.array(embeddings).astype('float32'))\n",
                "\n",
                "faiss.write_index(index, os.path.join(vector_store_dir, \"complaints.index\"))\n",
                "with open(os.path.join(vector_store_dir, \"metadata.pkl\"), \"wb\") as f:\n",
                "    pickle.dump(metadata, f)\n",
                "\n",
                "print(\"Vector store persisted successfully.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}